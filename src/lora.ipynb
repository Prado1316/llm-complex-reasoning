{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c049ac94-3a85-4c3b-8da5-9fabfd10114e",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install modelscope==1.9.5\n",
    "!pip install \"transformers>=4.39.0\"\n",
    "!pip install streamlit==1.24.0\n",
    "!pip install sentencepiece==0.1.99\n",
    "!pip install transformers_stream_generator==0.0.4\n",
    "!pip install datasets==2.18.0\n",
    "!pip install peft==0.10.0\n",
    "!pip install openai==1.17.1\n",
    "!pip install tqdm==4.64.1\n",
    "!pip install transformers==4.39.3\n",
    "!python -m pip install setuptools==69.5.1\n",
    "!pip install vllm==0.4.0.post1\n",
    "!pip install nest-asyncio\n",
    "!pip install accelerate\n",
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c111418c-1568-4ae4-8813-61c70865945e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from modelscope import snapshot_download, AutoModel, AutoTokenizer\n",
    "import os\n",
    "model_dir = snapshot_download('qwen/Qwen2-7B-Instruct', cache_dir='./', revision='master')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071ffac7-9617-4ad8-8df1-52a7b96576cf",
   "metadata": {},
   "source": [
    "## 这里运行完请重启notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8102f503-40ce-4132-b901-a2fcf8079f8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer, GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b226948-17c5-4aa4-8a81-06b61e93c425",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T15:24:55.871707Z",
     "iopub.status.busy": "2024-07-27T15:24:55.871404Z",
     "iopub.status.idle": "2024-07-27T15:24:55.920013Z",
     "shell.execute_reply": "2024-07-27T15:24:55.919573Z",
     "shell.execute_reply.started": "2024-07-27T15:24:55.871689Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 将JSON文件转换为CSV文件\n",
    "df = pd.read_json('data/train.json')\n",
    "ds = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7253ad6-95e1-48c5-8e24-5a90adc112fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T15:24:58.501761Z",
     "iopub.status.busy": "2024-07-27T15:24:58.501451Z",
     "iopub.status.idle": "2024-07-27T15:24:58.510477Z",
     "shell.execute_reply": "2024-07-27T15:24:58.510039Z",
     "shell.execute_reply.started": "2024-07-27T15:24:58.501743Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': ['你是一个逻辑推理专家，擅长解决逻辑推理问题。以下是一个逻辑推理的题目，形式为单项选择题。所有的问题都是（close-world assumption）闭世界假设，即未观测事实都为假。请逐步分析问题并在最后一行输出答案，最后一行的格式为\"答案是：A\"。题目如下：\\n\\n### 题目:\\n假设您需要构建一个二叉搜索树，其中每个节点或者是一个空的节点（称为\"空节点\"），或者是一个包含一个整数值和两个子树的节点（称为\"数值节点\"）。以下是构建这棵树的规则：\\n\\n1. 树中不存在重复的元素。\\n2. 对于每个数值节点，其左子树的所有值都小于该节点的值，其右子树的所有值都大于该节点的值。\\n3. 插入一个新值到一个\"空节点\"时，该\"空节点\"会被一个包含新值的新的数值节点取代。\\n4. 插入一个已存在的数值将不会改变树。\\n\\n请基于以上规则，回答以下选择题：\\n\\n### 问题:\\n选择题 1：\\n给定一个空的二叉搜索树，插入下列数字: [5, 9, 2, 10, 11, 3]，下面哪个选项正确描述了结果树的结构？\\nA. tree(5, tree(2, tree(3, nil, nil), nil), tree(9, tree(10, nil, nil), tree(11, nil, nil)))\\nB. tree(5, tree(2, nil, tree(3, nil, nil)), tree(9, nil, tree(10, nil, tree(11, nil, nil))))\\nC. tree(5, tree(3, tree(2, nil, nil), nil), tree(9, nil, tree(10, tree(11, nil, nil), nil)))\\nD. tree(5, nil, tree(2, nil, tree(3, nil, nil)), tree(9, tree(11, nil, nil), tree(10, nil, nil)))\\n',\n",
       "  '你是一个逻辑推理专家，擅长解决逻辑推理问题。以下是一个逻辑推理的题目，形式为单项选择题。所有的问题都是（close-world assumption）闭世界假设，即未观测事实都为假。请逐步分析问题并在最后一行输出答案，最后一行的格式为\"答案是：A\"。题目如下：\\n\\n### 题目:\\n**单项选择测试题**\\n\\n有一个函数可以从给定的列表中删除所有奇数，保留所有偶数。以下是该函数的逻辑描述：\\n\\n1. 如果列表为空，则返回空列表。\\n2. 对于列表中的每一个元素，如果该元素是奇数，则将其删除，继续处理列表中的下一个元素。\\n3. 如果该元素是偶数，则将其保留在结果列表中，并继续处理列表中的下一个元素。\\n\\n根据以上逻辑，回答以下选择题：\\n\\n### 问题:\\n选择题 3：\\n对于输入列表 [3,7,5,9]，函数返回的结果是什么？\\nA. [3, 7, 5, 9]\\nB. [0]\\nC. [7]\\nD. []\\n',\n",
       "  '你是一个逻辑推理专家，擅长解决逻辑推理问题。以下是一个逻辑推理的题目，形式为单项选择题。所有的问题都是（close-world assumption）闭世界假设，即未观测事实都为假。请逐步分析问题并在最后一行输出答案，最后一行的格式为\"答案是：A\"。题目如下：\\n\\n### 题目:\\n有一个函数用来计算斐波那契数列中的第N个数字。斐波那契数列定义如下：第0个数字是1，第1个数字也是1，从第2个数字开始，每个数字是其前两个数字之和。具体如下：\\n\\n1. 斐波那契数列的第0个数字是1。\\n2. 斐波那契数列的第1个数字是1。\\n3. 对于大于1的任何正整数N，第N个数字是第N-1个数字和第N-2个数字之和。\\n\\n根据上述定义，回答以下选择题：\\n\\n### 问题:\\n选择题 2：\\n斐波那契数列中的第4个数字是1吗？\\nA. 是\\nB. 不是\\n'],\n",
       " 'input': ['', '', ''],\n",
       " 'output': ['根据题目中的规则，我们依次插入数字 [5, 9, 2, 10, 11, 3] 来构建二叉搜索树。\\n\\n首先插入数字 5，树为：\\n```\\ntree(5, nil, nil)\\n```\\n\\n然后插入数字 9，由于 9 > 5，因此插入到右子树：\\n```\\ntree(5, nil, tree(9, nil, nil))\\n```\\n\\n接下来插入数字 2，由于 2 < 5，因此插入到左子树：\\n```\\ntree(5, tree(2, nil, nil), tree(9, nil, nil))\\n```\\n\\n再插入数字 10，由于 10 > 5 并且 10 > 9，所以插入到 9 的右子树：\\n```\\ntree(5, tree(2, nil, nil), tree(9, nil, tree(10, nil, nil)))\\n```\\n\\n继续插入数字 11，由于 11 > 5 并且 11 > 9 和 11 > 10，所以插入到 10 的右子树：\\n```\\ntree(5, tree(2, nil, nil), tree(9, nil, tree(10, nil, tree(11, nil, nil))))\\n```\\n\\n最后插入数字 3，由于 3 < 5 但是 3 > 2，所以插入到 2 的右子树：\\n```\\ntree(5, tree(2, nil, tree(3, nil, nil)), tree(9, nil, tree(10, nil, tree(11, nil, nil))))\\n```\\n\\n比较这个最终结果与给定的选项，可以看到选项 B 正确地描述了结果树的结构。\\n\\n答案是：B',\n",
       "  '根据题目描述的函数逻辑，我们来逐步分析输入列表 [3,7,5,9] 的处理过程：\\n\\n1. 列表非空，开始处理第一个元素3。\\n2. 3是奇数，根据规则应被删除。\\n3. 处理下一个元素7，同样为奇数，删除。\\n4. 继续处理5，同样是奇数，删除。\\n5. 最后处理9，依然是奇数，删除。\\n\\n整个过程中，由于输入列表中的所有元素都是奇数，根据规则它们都将被删除，不会有任何元素保留在结果列表中。\\n\\n因此，对于输入列表 [3,7,5,9]，函数返回的结果是空列表。\\n\\n答案是：D. []',\n",
       "  '为了回答这个问题，我们首先需要根据斐波那契数列的定义来计算前几个数字：\\n\\n1. 第0个数字是1。\\n2. 第1个数字是1。\\n3. 第2个数字是第0个数字（1）加上第1个数字（1），等于2。\\n4. 第3个数字是第1个数字（1）加上第2个数字（2），等于3。\\n5. 第4个数字是第2个数字（2）加上第3个数字（3），等于5。\\n\\n因此，斐波那契数列中的第4个数字是5，不是1。\\n\\n答案是：B. 不是'],\n",
       " '__index_level_0__': [0, 1, 2]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d94d5b5d-3aed-4e31-b475-318b94d5e922",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T15:24:59.126700Z",
     "iopub.status.busy": "2024-07-27T15:24:59.126411Z",
     "iopub.status.idle": "2024-07-27T15:24:59.314870Z",
     "shell.execute_reply": "2024-07-27T15:24:59.314270Z",
     "shell.execute_reply.started": "2024-07-27T15:24:59.126682Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2Tokenizer(name_or_path='./qwen/Qwen2-7B-Instruct', vocab_size=151643, model_max_length=131072, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('./qwen/Qwen2-7B-Instruct', use_fast=False, trust_remote_code=True)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f6deea5-d94f-4b23-88fa-2eb2d9f3f74a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T15:25:00.198564Z",
     "iopub.status.busy": "2024-07-27T15:25:00.198253Z",
     "iopub.status.idle": "2024-07-27T15:25:00.203028Z",
     "shell.execute_reply": "2024-07-27T15:25:00.202348Z",
     "shell.execute_reply.started": "2024-07-27T15:25:00.198547Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_func(example):\n",
    "    MAX_LENGTH = 1800    # Llama分词器会将一个中文字切分为多个token，因此需要放开一些最大长度，保证数据的完整性\n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "    instruction = tokenizer(f\"<|im_start|>system\\n你是一个逻辑推理专家，擅长解决逻辑推理问题。<|im_end|>\\n<|im_start|>user\\n{example['instruction'] + example['input']}<|im_end|>\\n<|im_start|>assistant\\n\", add_special_tokens=False)  # add_special_tokens 不在开头加 special_tokens\n",
    "    response = tokenizer(f\"{example['output']}\", add_special_tokens=False)\n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "    attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"] + [1]  # 因为eos token咱们也是要关注的所以 补充为1\n",
    "    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.pad_token_id]  \n",
    "    if len(input_ids) > MAX_LENGTH:  # 做一个截断\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cf900c4-9257-450b-bf07-0084fc46634f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T15:25:00.966985Z",
     "iopub.status.busy": "2024-07-27T15:25:00.966683Z",
     "iopub.status.idle": "2024-07-27T15:25:09.603337Z",
     "shell.execute_reply": "2024-07-27T15:25:09.602885Z",
     "shell.execute_reply.started": "2024-07-27T15:25:00.966967Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1204/1204 [00:06<00:00, 197.08 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 1204\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_id = ds.map(process_func, remove_columns=ds.column_names)\n",
    "tokenized_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c28a3b3-4688-42d2-9d18-55e6a00e70b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T15:25:11.405386Z",
     "iopub.status.busy": "2024-07-27T15:25:11.405053Z",
     "iopub.status.idle": "2024-07-27T15:25:11.414868Z",
     "shell.execute_reply": "2024-07-27T15:25:11.414381Z",
     "shell.execute_reply.started": "2024-07-27T15:25:11.405368Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\n你是一个逻辑推理专家，擅长解决逻辑推理问题。<|im_end|>\\n<|im_start|>user\\n你是一个逻辑推理专家，擅长解决逻辑推理问题。以下是一个逻辑推理的题目，形式为单项选择题。所有的问题都是（close-world assumption）闭世界假设，即未观测事实都为假。请逐步分析问题并在最后一行输出答案，最后一行的格式为\"答案是：A\"。题目如下：\\n\\n### 题目:\\n假设您需要构建一个二叉搜索树，其中每个节点或者是一个空的节点（称为\"空节点\"），或者是一个包含一个整数值和两个子树的节点（称为\"数值节点\"）。以下是构建这棵树的规则：\\n\\n1. 树中不存在重复的元素。\\n2. 对于每个数值节点，其左子树的所有值都小于该节点的值，其右子树的所有值都大于该节点的值。\\n3. 插入一个新值到一个\"空节点\"时，该\"空节点\"会被一个包含新值的新的数值节点取代。\\n4. 插入一个已存在的数值将不会改变树。\\n\\n请基于以上规则，回答以下选择题：\\n\\n### 问题:\\n选择题 1：\\n给定一个空的二叉搜索树，插入下列数字: [5, 9, 2, 10, 11, 3]，下面哪个选项正确描述了结果树的结构？\\nA. tree(5, tree(2, tree(3, nil, nil), nil), tree(9, tree(10, nil, nil), tree(11, nil, nil)))\\nB. tree(5, tree(2, nil, tree(3, nil, nil)), tree(9, nil, tree(10, nil, tree(11, nil, nil))))\\nC. tree(5, tree(3, tree(2, nil, nil), nil), tree(9, nil, tree(10, tree(11, nil, nil), nil)))\\nD. tree(5, nil, tree(2, nil, tree(3, nil, nil)), tree(9, tree(11, nil, nil), tree(10, nil, nil)))\\n<|im_end|>\\n<|im_start|>assistant\\n根据题目中的规则，我们依次插入数字 [5, 9, 2, 10, 11, 3] 来构建二叉搜索树。\\n\\n首先插入数字 5，树为：\\n```\\ntree(5, nil, nil)\\n```\\n\\n然后插入数字 9，由于 9 > 5，因此插入到右子树：\\n```\\ntree(5, nil, tree(9, nil, nil))\\n```\\n\\n接下来插入数字 2，由于 2 < 5，因此插入到左子树：\\n```\\ntree(5, tree(2, nil, nil), tree(9, nil, nil))\\n```\\n\\n再插入数字 10，由于 10 > 5 并且 10 > 9，所以插入到 9 的右子树：\\n```\\ntree(5, tree(2, nil, nil), tree(9, nil, tree(10, nil, nil)))\\n```\\n\\n继续插入数字 11，由于 11 > 5 并且 11 > 9 和 11 > 10，所以插入到 10 的右子树：\\n```\\ntree(5, tree(2, nil, nil), tree(9, nil, tree(10, nil, tree(11, nil, nil))))\\n```\\n\\n最后插入数字 3，由于 3 < 5 但是 3 > 2，所以插入到 2 的右子树：\\n```\\ntree(5, tree(2, nil, tree(3, nil, nil)), tree(9, nil, tree(10, nil, tree(11, nil, nil))))\\n```\\n\\n比较这个最终结果与给定的选项，可以看到选项 B 正确地描述了结果树的结构。\\n\\n答案是：B<|endoftext|>'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_id[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a3325fb-487b-4dd6-9fdb-8a7abbf5b8b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T15:25:13.652481Z",
     "iopub.status.busy": "2024-07-27T15:25:13.652174Z",
     "iopub.status.idle": "2024-07-27T15:25:13.657627Z",
     "shell.execute_reply": "2024-07-27T15:25:13.657204Z",
     "shell.execute_reply.started": "2024-07-27T15:25:13.652462Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'根据题目描述的函数逻辑，我们来逐步分析输入列表 [3,7,5,9] 的处理过程：\\n\\n1. 列表非空，开始处理第一个元素3。\\n2. 3是奇数，根据规则应被删除。\\n3. 处理下一个元素7，同样为奇数，删除。\\n4. 继续处理5，同样是奇数，删除。\\n5. 最后处理9，依然是奇数，删除。\\n\\n整个过程中，由于输入列表中的所有元素都是奇数，根据规则它们都将被删除，不会有任何元素保留在结果列表中。\\n\\n因此，对于输入列表 [3,7,5,9]，函数返回的结果是空列表。\\n\\n答案是：D. []<|endoftext|>'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(list(filter(lambda x: x != -100, tokenized_id[1][\"labels\"])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d514a730-3af3-444c-94f3-9a03a6f1ccf2",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-27T15:25:14.373031Z",
     "iopub.status.busy": "2024-07-27T15:25:14.372721Z",
     "iopub.status.idle": "2024-07-27T15:25:58.435349Z",
     "shell.execute_reply": "2024-07-27T15:25:58.434861Z",
     "shell.execute_reply.started": "2024-07-27T15:25:14.373014Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:40<00:00, 10.20s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(152064, 3584)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
       "          (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "          (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "          (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "          (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm()\n",
       "        (post_attention_layernorm): Qwen2RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('./qwen/Qwen2-7B-Instruct', device_map=\"auto\",torch_dtype=torch.bfloat16)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51fe0c77-a538-45b8-aa9f-14f3ef2306cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T15:25:58.436541Z",
     "iopub.status.busy": "2024-07-27T15:25:58.436277Z",
     "iopub.status.idle": "2024-07-27T15:25:58.439911Z",
     "shell.execute_reply": "2024-07-27T15:25:58.439408Z",
     "shell.execute_reply.started": "2024-07-27T15:25:58.436522Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.enable_input_require_grads() # 开启梯度检查点时，要执行该方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5dca344-f91f-4f98-94a6-c742bdaf15e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T15:25:58.440826Z",
     "iopub.status.busy": "2024-07-27T15:25:58.440621Z",
     "iopub.status.idle": "2024-07-27T15:25:58.445365Z",
     "shell.execute_reply": "2024-07-27T15:25:58.444921Z",
     "shell.execute_reply.started": "2024-07-27T15:25:58.440812Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.bfloat16"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f51b7493-83d9-4489-bdb9-fdac20386dae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T15:25:58.446664Z",
     "iopub.status.busy": "2024-07-27T15:25:58.446479Z",
     "iopub.status.idle": "2024-07-27T15:25:58.450428Z",
     "shell.execute_reply": "2024-07-27T15:25:58.449977Z",
     "shell.execute_reply.started": "2024-07-27T15:25:58.446650Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, inference_mode=False, r=8, target_modules={'down_proj', 'v_proj', 'gate_proj', 'up_proj', 'k_proj', 'q_proj', 'o_proj'}, lora_alpha=32, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, \n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    inference_mode=False, # 训练模式\n",
    "    r=8, # Lora 秩\n",
    "    lora_alpha=32, # Lora alaph，具体作用参见 Lora 原理\n",
    "    lora_dropout=0.1# Dropout 比例\n",
    ")\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eff0aed7-9726-45a8-be09-1e20b9342838",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T15:25:58.451160Z",
     "iopub.status.busy": "2024-07-27T15:25:58.450958Z",
     "iopub.status.idle": "2024-07-27T15:25:59.162789Z",
     "shell.execute_reply": "2024-07-27T15:25:59.162196Z",
     "shell.execute_reply.started": "2024-07-27T15:25:58.451144Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='./qwen/Qwen2-7B-Instruct', revision=None, task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, inference_mode=False, r=8, target_modules={'down_proj', 'v_proj', 'gate_proj', 'up_proj', 'k_proj', 'q_proj', 'o_proj'}, lora_alpha=32, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_peft_model(model, config)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3a74f48-3823-4ab8-8362-4a37416d66fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T15:25:59.163725Z",
     "iopub.status.busy": "2024-07-27T15:25:59.163490Z",
     "iopub.status.idle": "2024-07-27T15:25:59.170009Z",
     "shell.execute_reply": "2024-07-27T15:25:59.169528Z",
     "shell.execute_reply.started": "2024-07-27T15:25:59.163708Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.26434798934534914\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "679a6e7d-e85d-4a6b-96fa-0deb601b62e0",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-27T15:25:59.170858Z",
     "iopub.status.busy": "2024-07-27T15:25:59.170656Z",
     "iopub.status.idle": "2024-07-27T15:25:59.182887Z",
     "shell.execute_reply": "2024-07-27T15:25:59.182488Z",
     "shell.execute_reply.started": "2024-07-27T15:25:59.170844Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"./output/Qwen2_instruct_lora\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=1,\n",
    "    save_steps=100, # 为了快速演示，这里设置10，建议你设置成100\n",
    "    learning_rate=1e-4,\n",
    "    save_on_each_node=True,\n",
    "    gradient_checkpointing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b90ef0a-7bfb-4252-a899-6b2c44972a9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T15:25:59.183628Z",
     "iopub.status.busy": "2024-07-27T15:25:59.183447Z",
     "iopub.status.idle": "2024-07-27T15:25:59.235050Z",
     "shell.execute_reply": "2024-07-27T15:25:59.234596Z",
     "shell.execute_reply.started": "2024-07-27T15:25:59.183614Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.19.91, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_id,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20463028-64c9-4ebe-98a9-512fe3530593",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T15:25:59.235807Z",
     "iopub.status.busy": "2024-07-27T15:25:59.235631Z",
     "iopub.status.idle": "2024-07-27T15:25:59.238230Z",
     "shell.execute_reply": "2024-07-27T15:25:59.237704Z",
     "shell.execute_reply.started": "2024-07-27T15:25:59.235793Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.backends.cuda.enable_mem_efficient_sdp(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b59bf7e-7d47-4973-bd6a-718da07cc650",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T15:26:18.753267Z",
     "iopub.status.busy": "2024-07-27T15:26:18.752959Z",
     "iopub.status.idle": "2024-07-27T15:37:03.079543Z",
     "shell.execute_reply": "2024-07-27T15:37:03.079094Z",
     "shell.execute_reply.started": "2024-07-27T15:26:18.753249Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-27 23:26:19,017] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='301' max='301' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [301/301 10:40, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.670800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.574900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.626100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.591700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.575300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.644400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.577500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.601700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.485900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.583000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.592700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.540600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.597700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.509000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.620700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.530400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.519800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.576700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.528600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.609000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.604700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.569100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.603400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.555200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.542100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.521900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.625300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.523800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.539200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.554900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ./qwen/Qwen2-7B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ./qwen/Qwen2-7B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ./qwen/Qwen2-7B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ./qwen/Qwen2-7B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=301, training_loss=0.5728449522458834, metrics={'train_runtime': 643.6899, 'train_samples_per_second': 1.87, 'train_steps_per_second': 0.468, 'total_flos': 2.335115283830784e+16, 'train_loss': 0.5728449522458834, 'epoch': 1.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a79d04-7a2a-4711-8f22-c599a0fe6d65",
   "metadata": {},
   "source": [
    "## 这里运行完请重启notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "743e2e1b-3108-432e-be38-2597332221cf",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-27T16:15:26.199740Z",
     "iopub.status.busy": "2024-07-27T16:15:26.199558Z",
     "iopub.status.idle": "2024-07-27T16:16:24.698704Z",
     "shell.execute_reply": "2024-07-27T16:16:24.698195Z",
     "shell.execute_reply.started": "2024-07-27T16:15:26.199723Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-07-28 00:15:33.460211: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-28 00:15:33.658560: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-28 00:15:33.751489: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-28 00:15:33.774577: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-28 00:15:33.923242: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-28 00:15:34.902958: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:40<00:00, 10.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "\n",
    "mode_path = './qwen/Qwen2-7B-Instruct/'\n",
    "lora_path = './output/Qwen2_instruct_lora_an/checkpoint-100' # 这里改称你的 lora 输出对应 checkpoint 地址\n",
    "\n",
    "# 加载tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(mode_path, trust_remote_code=True)\n",
    "\n",
    "# 加载模型\n",
    "model = AutoModelForCausalLM.from_pretrained(mode_path, device_map=\"auto\",torch_dtype=torch.float16, trust_remote_code=True).eval()\n",
    "\n",
    "# 加载lora权重\n",
    "model = PeftModel.from_pretrained(model, model_id=lora_path)\n",
    "\n",
    "prompt = '''你是一个逻辑推理专家，擅长解决逻辑推理问题。以下是一个逻辑推理的题目，形式为单项选择题。所有的问题都是（close-world assumption）闭世界假设，即未观测事实都为假。请逐步分析问题并在最后一行输出答案，最后一行的格式为\"答案是：A\"。题目如下：\\n\\n### 题目:\\n假设您需要构建一个二叉搜索树，其中每个节点或者是一个空的节点（称为\"空节点\"），或者是一个包含一个整数值和两个子树的节点（称为\"数值节点\"）。以下是构建这棵树的规则：\\n\\n1. 树中不存在重复的元素。\\n2. 对于每个数值节点，其左子树的所有值都小于该节点的值，其右子树的所有值都大于该节点的值。\\n3. 插入一个新值到一个\"空节点\"时，该\"空节点\"会被一个包含新值的新的数值节点取代。\\n4. 插入一个已存在的数值将不会改变树。\\n\\n请基于以上规则，回答以下选择题：\\n\\n### 问题:\\n选择题 1：\\n给定一个空的二叉搜索树，插入下列数字: [5, 9, 2, 10, 11, 3]，下面哪个选项正确描述了结果树的结构？\\nA. tree(5, tree(2, tree(3, nil, nil), nil), tree(9, tree(10, nil, nil), tree(11, nil, nil)))\\nB. tree(5, tree(2, nil, tree(3, nil, nil)), tree(9, nil, tree(10, nil, tree(11, nil, nil))))\\nC. tree(5, tree(3, tree(2, nil, nil), nil), tree(9, nil, tree(10, tree(11, nil, nil), nil)))\\nD. tree(5, nil, tree(2, nil, tree(3, nil, nil)), tree(9, tree(11, nil, nil), tree(10, nil, nil)))'''\n",
    "inputs = tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": \"你是一个逻辑推理专家，擅长解决逻辑推理问题。\"},{\"role\": \"user\", \"content\": prompt}],\n",
    "                                       add_generation_prompt=True,\n",
    "                                       tokenize=True,\n",
    "                                       return_tensors=\"pt\",\n",
    "                                       return_dict=True\n",
    "                                       ).to('cuda')\n",
    "\n",
    "\n",
    "gen_kwargs = {\"max_length\": 2500, \"do_sample\": True, \"top_k\": 1}\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, **gen_kwargs)\n",
    "    outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfa947a0-9830-4d21-8b3d-3e0116ec13c1",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-27T16:16:39.950973Z",
     "iopub.status.busy": "2024-07-27T16:16:39.950661Z",
     "iopub.status.idle": "2024-07-27T16:17:16.352942Z",
     "shell.execute_reply": "2024-07-27T16:17:16.352287Z",
     "shell.execute_reply.started": "2024-07-27T16:16:39.950954Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 模型合并存储\n",
    "\n",
    "new_model_directory = \"./merged_model_an\"\n",
    "merged_model = model.merge_and_unload()\n",
    "# 将权重保存为safetensors格式的权重, 且每个权重文件最大不超过2GB(2048MB)\n",
    "merged_model.save_pretrained(new_model_directory, max_shard_size=\"2048MB\", safe_serialization=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e6efba9-0d26-4e5c-908b-df42ea78c5c8",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-27T16:17:32.549954Z",
     "iopub.status.busy": "2024-07-27T16:17:32.549625Z",
     "iopub.status.idle": "2024-07-27T16:17:34.724805Z",
     "shell.execute_reply": "2024-07-27T16:17:34.724217Z",
     "shell.execute_reply.started": "2024-07-27T16:17:32.549935Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!cp ./qwen/Qwen2-7B-Instruct/tokenizer.json ./merged_model_an/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cd2323-d732-4e15-963b-c51e1975401f",
   "metadata": {},
   "source": [
    "## 这里运行完请重启notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b4e63f-1488-4417-8060-b68a65b64803",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
